{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0833cd7-18b5-4ac4-9454-03464e0bb10c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q google-cloud-aiplatform==1.36.0 langchain==0.0.327 unstructured chromadb==0.4.15 --upgrade --user \"unstructured[pdf]\" pydantic gradio pdf2image google-cloud-vision google-cloud-storage pymupdf easyocr numpy python-docx python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea9913f-bea4-47d8-8c55-6e5965df9bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d39d7-ee12-431b-918c-1485e682c266",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f640c417-432d-4edd-9d54-0006e0ee9663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# # Chroma DB as Vector Store Database\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44daee6d-8df8-4bb9-8a53-6b6f9c9a5e39",
   "metadata": {},
   "source": [
    "# Global vars & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7e0acb-df76-4dba-81a7-5248dbb1aac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 19:28:12.566513: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 19:28:13.852622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-09-25 19:28:13.852778: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-09-25 19:28:13.852788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"chroma_db5\"\n",
    "CHROMA_DB_CLIENT = chromadb.PersistentClient(path = DB_PATH)\n",
    "QA= None\n",
    "CACHE_COLLECTION_NAME = None\n",
    "\n",
    "EMBEDDING = VertexAIEmbeddings()\n",
    "# EMBEDDING = VertexAIEmbeddings(model_name='text-embedding-004')\n",
    "LLM = VertexAI(\n",
    "    model_name=\"text-bison-32k\",\n",
    "    # model_name = \"gemini-1.0-pro\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.6,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda20da-e09d-43aa-bc4c-6f6cf7196f3e",
   "metadata": {},
   "source": [
    "# Retrive Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4279667e-adfa-4876-8820-99bd48a88b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_retrieval_qa(collection_name):\n",
    "    db = Chroma(persist_directory=DB_PATH, collection_name=collection_name, embedding_function=EMBEDDING)\n",
    "    # retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "    retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "    qa = RetrievalQA.from_chain_type(llm=LLM, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    return qa\n",
    "\n",
    "def process_question(question, collection_name):\n",
    "    global QA, CACHE_COLLECTION_NAME\n",
    "    try:\n",
    "        if (not CACHE_COLLECTION_NAME) or (collection_name!=CACHE_COLLECTION_NAME):\n",
    "            QA = get_retrieval_qa(collection_name)\n",
    "            CACHE_COLLECTION_NAME = collection_name\n",
    "        response = QA({\"query\": question})\n",
    "        return response[\"result\"], response[\"source_documents\"][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {e}\")\n",
    "        return \"Error processing question\", []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365024e-8ee9-406d-b932-0a4794f32971",
   "metadata": {},
   "source": [
    "# Vector DB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cea057-18a1-42dc-9c82-51c801adf3de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_collections():\n",
    "    # Function to get the list of all collections\n",
    "    collections = CHROMA_DB_CLIENT.list_collections()\n",
    "    collection_names = [col.name for col in collections]\n",
    "    return collection_names\n",
    "\n",
    "def get_files_in_collection(collection_name):\n",
    "    # Function to get the list of files in a given collection\n",
    "    try:\n",
    "        collection = CHROMA_DB_CLIENT.get_collection(collection_name)\n",
    "        documents = collection.get()\n",
    "        \n",
    "        # Prepare a list to store file details\n",
    "        file_details = {}\n",
    "        \n",
    "        for i in range(len(documents['ids'])):\n",
    "            file_details[documents['metadatas'][i]['file_name']] = {'Upload Date Time': documents['metadatas'][i]['upload_date_time']}\n",
    "        \n",
    "        return file_details\n",
    "    \n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46962c22-0026-40a7-9b12-1a153c8efdca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UI: Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afe195-f784-426b-a5ab-950234ab4323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://1224ea2fe39f5216f9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1224ea2fe39f5216f9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# OCR_based_gpt\")\n",
    "    gr.Markdown(\"Interact with your chatbot here! Ask questions based on the content of the files.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # Q&A Section\n",
    "            gr.Markdown(\"## Ask Your Question\")\n",
    "            \n",
    "            # Inputs for the question and collection name\n",
    "            question_input = gr.Textbox(label=\"Enter your question\")\n",
    "            collection_input = gr.Textbox(label=\"Collection Name\", placeholder=\"Enter Collection Name\")\n",
    "            \n",
    "            # Button to submit the question\n",
    "            submit_btn = gr.Button(\"Submit Question\")\n",
    "            \n",
    "            # Outputs for the response and source document\n",
    "            response_output = gr.Textbox(label=\"Chatbot Response\")\n",
    "            source_doc_output = gr.JSON(label=\"Source Document\")\n",
    "            \n",
    "            submit_btn.click(\n",
    "                process_question,\n",
    "                inputs=[question_input, collection_input],\n",
    "                outputs=[response_output, source_doc_output]\n",
    "            )\n",
    "    \n",
    "        with gr.Column():\n",
    "            # List Collections Section\n",
    "            gr.Markdown(\"## Get All Collections\")\n",
    "            all_collections_output = gr.JSON(label=\"All Collections\")\n",
    "            gr.Button(\"List All Collections\").click(\n",
    "                get_all_collections,\n",
    "                inputs=[],\n",
    "                outputs=all_collections_output\n",
    "            )\n",
    "        \n",
    "            # List Files in Collection Section\n",
    "            gr.Markdown(\"## Get Files in Collection\")\n",
    "            collection_name_input = gr.Textbox(label=\"Collection Name\", placeholder=\"Enter Collection Name\")\n",
    "            files_output = gr.JSON(label=\"Files in Collection\")\n",
    "            gr.Button(\"List Files in Collection\").click(\n",
    "                get_files_in_collection,\n",
    "                inputs=collection_name_input,\n",
    "                outputs=files_output\n",
    "            )\n",
    "\n",
    "# Launch the Gradio app\n",
    "app.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738faf38-fe3e-4286-921a-c34d6115d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a9ab6-d3e9-49d8-afbf-05b21d5a3e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4519941-e51e-4a16-a4e8-9231ff709fc9",
   "metadata": {},
   "source": [
    "Architecture Diagram of OCR based chatbot? \n",
    "\n",
    "What ChromaDB client? \n",
    "\n",
    "What is text_splitter? \n",
    "\n",
    "What is upload and process? \n",
    "\n",
    "What is delete collection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd569d7-1a05-4126-94be-bfbd6a845dc7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
